{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stage1_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpxvQ14HFYM1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OIJwvszjF4AZ"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCBxNisC4Ga-"
      },
      "source": [
        "!pip install simpletransformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsmkWna44Fdc"
      },
      "source": [
        "project_path = '/content/drive/My Drive/Technical Debt/Codes/BERT/'## we will store our data in this drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4SSbaYMVgR7"
      },
      "source": [
        "#################\n",
        "###########\n",
        "#####\n",
        "\n",
        "\n",
        "output_directory_name = \"myoutput/\"\n",
        "\n",
        "\n",
        "#################\n",
        "###########\n",
        "#####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tg0tjgVC4JKu"
      },
      "source": [
        "def clean_text(text):\n",
        "    \n",
        "    text = re.sub(r\"\\n\", \" \", text)\n",
        "    text = re.sub(r\"\\r\", \" \", text)\n",
        "    \n",
        "\n",
        "\n",
        "    return text\n",
        "\n",
        "'''\n",
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import re, SnowballStemmer\n",
        "\n",
        "def clean_text(text):\n",
        "    import nltk\n",
        "    nltk.download('stopwords',quiet=True)\n",
        "    translate_table = dict((ord(char), None) for char in string.punctuation)\n",
        "    text = text.translate(translate_table)\n",
        "\n",
        "    re_url = re.compile(r\"((http|https)\\:\\/\\/)?[a-zA-Z0-9\\.\\/\\?\\:@\\-_=#]+\\\n",
        "                        .([a-zA-Z]){2,6}([a-zA-Z0-9\\.\\&\\/\\?\\:@\\-_=#])*\",\n",
        "                        re.MULTILINE | re.UNICODE)\n",
        "    re_ip = re.compile(\"\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\")\n",
        "\n",
        "    text = re_url.sub(\"URL\", text)\n",
        "\n",
        "    text = re_ip.sub(\"IPADDRESS\", text)\n",
        "\n",
        "    text = text.lower().split()\n",
        "\n",
        "    stops = set(stopwords.words(\"english\"))\n",
        "    text = [w for w in text if not w in stops and len(w) >= 3]\n",
        "\n",
        "    text = \" \".join(text)\n",
        "\n",
        "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
        "    text = re.sub(r\"what's\", \"what is \", text)\n",
        "    text = re.sub(r\"\\'s\", \" \", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text = re.sub(r\"n't\", \" not \", text)\n",
        "    text = re.sub(r\"i'm\", \"i am \", text)\n",
        "    text = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text = re.sub(r\",\", \" \", text)\n",
        "    text = re.sub(r\"\\.\", \" \", text)\n",
        "    text = re.sub(r\"!\", \" ! \", text)\n",
        "    text = re.sub(r\"\\/\", \" \", text)\n",
        "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
        "    text = re.sub(r\"\\+\", \" + \", text)\n",
        "    text = re.sub(r\"\\-\", \" - \", text)\n",
        "    text = re.sub(r\"\\=\", \" = \", text)\n",
        "    text = re.sub(r\"'\", \" \", text)\n",
        "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
        "    text = re.sub(r\":\", \" : \", text)\n",
        "    text = re.sub(r\" e g \", \" eg \", text)\n",
        "    text = re.sub(r\" b g \", \" bg \", text)\n",
        "    text = re.sub(r\" u s \", \" american \", text)\n",
        "    text = re.sub(r\"\\0s\", \"0\", text)\n",
        "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
        "    text = re.sub(r\"e - mail\", \"email\", text)\n",
        "    text = re.sub(r\"j k\", \"jk\", text)\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
        "    \n",
        "    text = text.split()\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    stemmed_words = [stemmer.stem(word) for word in text]\n",
        "    text = \" \".join(stemmed_words)\n",
        "\n",
        "\n",
        "    return text'''"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HLkBop16E6J"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "\n",
        "\n",
        "df_total=pd.read_excel(project_path+'TD_dataset.xlsx')\n",
        "df_total = df_total[['Sentence Text','TD (Sentence Level)']]\n",
        "df_total = df_total.rename({'Sentence Text': 'text', 'TD (Sentence Level)': 'labels'}, axis='columns')\n",
        "\n",
        "\n",
        "df_total['text']=df_total['text'].apply(str)\n",
        "\n",
        "df_total['text']=df_total['text'].map(lambda x: clean_text(x))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "df_train, df_test = train_test_split(df_total, test_size=0.2)\n",
        "\n",
        "\n",
        "\n",
        "texts_train=[]\n",
        "texts_train=df_train['text']\n",
        "\n",
        "label_train=df_train['labels']\n",
        "\n",
        "\n",
        "X_train=texts_train.astype(str).values\n",
        "X_train=np.reshape(X_train,(-1,1))\n",
        "\n",
        "\n",
        "label_train=label_train.astype(int).values\n",
        "y_train=np.reshape(label_train,(-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVPvYmEk-oHq"
      },
      "source": [
        "#print(num_of_TD_types)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbfZv7wQ58vN"
      },
      "source": [
        "#df_test=pd.read_csv(project_path+'test.csv')\n",
        "\n",
        "texts_test=[]\n",
        "texts_test=df_test['text']\n",
        "label_test=df_test['labels']\n",
        "\n",
        "\n",
        "X_test=texts_test.astype(str).values\n",
        "X_test=np.reshape(X_test,(-1,1))\n",
        "\n",
        "\n",
        "label_test=label_test.astype(int).values\n",
        "y_test=np.reshape(label_test,(-1,1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cyxuaPe4bWT"
      },
      "source": [
        "sentences = X_train[:,0]\n",
        "print(sentences[0])\n",
        "labels = y_train[:, 0]\n",
        "#labels = y_train\n",
        "print(labels[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ITPUk5tQ4QvR"
      },
      "source": [
        "max_len = max([len(sen.split(' ')) for sen in sentences])\n",
        "print('Max sentence length: ', max_len)\n",
        "\n",
        "total_len = 0\n",
        "\n",
        "for sen in sentences:\n",
        "  total_len += len(sen.split())\n",
        "\n",
        "print(\"Avg len:\")\n",
        "print(total_len/len(sentences))\n",
        "\n",
        "'''\n",
        "large_count = 0\n",
        "taken_sentence = []\n",
        "for sen in sentences:\n",
        "  if len(sen.split(' ')) != max_len:\n",
        "    taken_sentence.append(sen)\n",
        "\n",
        "sentences = taken_sentence\n",
        "del taken_sentence\n",
        "\n",
        "total_len = 0\n",
        "\n",
        "for sen in sentences:\n",
        "  total_len += len(sen.split())\n",
        "\n",
        "print(\"Avg len:\")\n",
        "print(total_len/len(sentences))'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33g5TEkldF8K"
      },
      "source": [
        "VALIDATION_SPLIT = 0.2\n",
        "\n",
        "nb_validation_samples = int(VALIDATION_SPLIT * sentences.shape[0])\n",
        "\n",
        "x_train = sentences[:-nb_validation_samples]\n",
        "y_train = labels[:-nb_validation_samples]\n",
        "x_val = sentences[-nb_validation_samples:]\n",
        "y_val = labels[-nb_validation_samples:]\n",
        "\n",
        "print(x_train[0])\n",
        "print(x_val[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_fzadJpd7gaw"
      },
      "source": [
        "import pandas as pd\n",
        "ll=[]\n",
        "for i in range(len(x_train)):\n",
        "  ll.append([x_train[i],y_train[i]])\n",
        "\n",
        "train_df = pd.DataFrame(ll,columns=['text','labels'])\n",
        "\n",
        "\n",
        "ll=[]\n",
        "for i in range(len(x_val)):\n",
        "  ll.append([x_val[i],y_val[i]])\n",
        "\n",
        "val_df = pd.DataFrame(ll,columns=['text','labels'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2dAnwQvhlUm"
      },
      "source": [
        "#######################\n",
        "\n",
        "train_df = train_df.append(val_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frj6xSKu_w2y"
      },
      "source": [
        "sentences_test = X_test[:,0]\n",
        "print(sentences_test[0])\n",
        "labels_test = y_test[:, 0]\n",
        "#labels = y_train\n",
        "print(labels_test[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iU-lAE0N8Uc0"
      },
      "source": [
        "ll=[]\n",
        "for i in range(len(sentences_test)):\n",
        "  ll.append([sentences_test[i],labels_test[i]])\n",
        "\n",
        "test_df = pd.DataFrame(ll,columns=['text','labels'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHPzg87wemnb"
      },
      "source": [
        "print('Training sample: ' + str(len(train_df['text'])))\n",
        "print('Validation sample: ' + str(len(val_df['text'])))\n",
        "print('Testing sample: ' + str(len(test_df['text'])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMRne0wQSR63"
      },
      "source": [
        "del val_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "evd3WkX45UPc"
      },
      "source": [
        "model_type = \"bert\"\n",
        "model_name = \"bert-base-cased\"\n",
        "\n",
        "train_args = {\n",
        "    \"reprocess_input_data\": True,\n",
        "    \"overwrite_output_dir\": True,\n",
        "    \"use_cached_eval_features\": True,\n",
        "    \"output_dir\": project_path+output_directory_name+model_type,\n",
        "    \"best_model_dir\": project_path+output_directory_name+model_type+\"/best_model\",\n",
        "    \"use_early_stopping\": False,\n",
        "    \"early_stopping_delta\": 0.0,\n",
        "    \"early_stopping_metric\": \"eval_loss\",\n",
        "    \"early_stopping_metric_minimize\" : True,\n",
        "    \"early_stopping_patience\" : 2,\n",
        "    \"evaluate_during_training\": True,\n",
        "    \"max_seq_length\": 512,\n",
        "    \"num_train_epochs\": 20,\n",
        "    \"evaluate_during_training_steps\": 64,\n",
        "    \"wandb_project\": \"Fake News Liar_1 Bert\",\n",
        "    \"wandb_kwargs\": {\"name\": model_name},\n",
        "    \"save_model_every_epoch\": False,\n",
        "    \"save_eval_checkpoints\": False,\n",
        "    \"train_batch_size\": 64,\n",
        "    \"eval_batch_size\": 64,\n",
        "    \"evaluate_during_training_verbose\" : True\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Cb8Nn9-65k6"
      },
      "source": [
        "from simpletransformers.classification import ClassificationModel, ClassificationArgs\n",
        "#import pandas as pd\n",
        "\n",
        "train_args[\"max_seq_length\"] = 300\n",
        "train_args[\"train_batch_size\"] = 32\n",
        "train_args[\"gradient_accumulation_steps\"] = 1\n",
        "train_args[\"evaluate_during_training\"] = False\n",
        "train_args[\"evaluate_during_training_steps\"] = 256\n",
        "train_args[\"num_train_epochs\"] = 10\n",
        "train_args[\"use_early_stopping\"] = True\n",
        "\n",
        "\n",
        "# Create a ClassificationModel\n",
        "model = ClassificationModel(model_type, model_name, args=train_args)\n",
        "\n",
        "# Train the model\n",
        "model.train_model(train_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJhJrQnc-8T1"
      },
      "source": [
        "result, model_outputs, wrong_predictions = model.eval_model(test_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbTJrCnfWssH"
      },
      "source": [
        "predictions, raw_outputs = model.predict(test_df['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tTzHVMsxXTzG"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print(test_df['labels'].tolist())\n",
        "print(predictions)\n",
        "test_df['pred']=predictions\n",
        "\n",
        "print(accuracy_score(test_df['labels'],predictions))\n",
        "\n",
        "print(test_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m_eEPfXxTUU"
      },
      "source": [
        "import collections\n",
        "\n",
        "\n",
        "print(collections.Counter(train_df['labels'].tolist()))\n",
        "print(collections.Counter(test_df['labels'].tolist()))\n",
        "print(collections.Counter(predictions))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAkN1EnLzvyO"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "print(confusion_matrix(test_df['labels'].tolist(),predictions,labels=[0,1,2,3,4,5,6,7,8,9]))\n",
        "\n",
        "#'DOCUMENTATION':0, 'BUILD':1, 'REQUIREMENT':2, 'ARCHITECTURE':3, 'DESIGN':4, 'USABILITY':5, 'CODE':6, 'VERSIONING':7, 'TEST':8, 'DEFECT':9"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tphUDGNZVIF"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print(accuracy_score(y_test, predictions))\n",
        "\n",
        "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
        "print(classification_report(y_test, predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMB76UTwh3MU"
      },
      "source": [
        "train_df.to_pickle(project_path+'Train_Test_Data/train_df.pkl')\n",
        "test_df.to_pickle(project_path+'Train_Test_Data/test_df.pkl')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}