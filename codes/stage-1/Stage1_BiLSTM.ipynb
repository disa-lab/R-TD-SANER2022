{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Stage1_BiLSTM.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zv6lr6IkSJIo"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6YWv0p1cSKLZ"
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFmYNmG7SL1g"
      },
      "source": [
        "project_path = '/content/drive/My Drive/Technical Debt/Codes/BiLSTM/'## we will store our data in this drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFhfzGoEScYY"
      },
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import re, SnowballStemmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVGMKQMzT0Mp"
      },
      "source": [
        "##building bi-lstm architecture\n",
        "\n",
        "from keras import callbacks\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation,Flatten,Dense,Dropout,Embedding,Bidirectional,LSTM\n",
        "\n",
        "\n",
        "\n",
        "def create_model(vocabulary_size,embedding_size,embedding_matrix):\n",
        "    model_glove = Sequential()\n",
        "    model_glove.add(Embedding(vocabulary_size, embedding_size, weights=[embedding_matrix], trainable=False))\n",
        "    model_glove.add(Bidirectional(LSTM(100)))\n",
        "    model_glove.add(Dense(1, activation='sigmoid'))\n",
        "    model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    model_glove.summary()\n",
        "    return model_glove\n",
        "\n",
        "\n",
        "def callback(model_name,tf_log_dir_name='./tf-log/',patience_lr=10,):\n",
        "    cb = []\n",
        "    \"\"\"\n",
        "    Tensorboard log callback\n",
        "    \"\"\"\n",
        "    tb = callbacks.TensorBoard(log_dir=tf_log_dir_name, histogram_freq=0)\n",
        "    cb.append(tb)\n",
        "\n",
        "    \"\"\"\n",
        "    Model-Checkpoint\n",
        "    \"\"\"\n",
        "    m = callbacks.ModelCheckpoint(filepath=model_name,monitor='val_loss',mode='auto',save_best_only=True)\n",
        "    cb.append(m)\n",
        "\n",
        "    \"\"\"\n",
        "    Reduce Learning Rate\n",
        "    \"\"\"\n",
        "    reduce_lr_loss = callbacks.ReduceLROnPlateau(monitor='loss', factor=0.1, patience=patience_lr, verbose=1, epsilon=1e-4, mode='min')\n",
        "    cb.append(reduce_lr_loss)\n",
        "\n",
        "    \"\"\"\n",
        "    Early Stopping callback\n",
        "    \"\"\"\n",
        "    # Uncomment for usage\n",
        "    early_stop = callbacks.EarlyStopping(monitor='val_acc', min_delta=0, patience=5, verbose=1, mode='auto')\n",
        "    cb.append(early_stop)\n",
        "\n",
        "\n",
        "\n",
        "    return cb\n",
        "\n",
        "######### Show Train Val History Graph ###############\n",
        "def plot_loss_accu(history,lossLoc='Train_Val_Loss',accLoc='Train_Val_acc'):\n",
        "    import matplotlib.pyplot as plt\n",
        "\n",
        "    plt.clf()\n",
        "\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs = range(len(loss))\n",
        "    plt.plot(epochs, loss, 'r')\n",
        "    plt.plot(epochs, val_loss, 'b')\n",
        "    plt.title('Training and validation loss')\n",
        "    plt.legend(['train', 'val'], loc='upper right')\n",
        "    #plt.show()\n",
        "    plt.savefig(lossLoc)\n",
        "\n",
        "    plt.clf()\n",
        "\n",
        "    acc = history.history['acc']\n",
        "    val_acc = history.history['val_acc']\n",
        "    epochs = range(len(acc))\n",
        "    plt.plot(epochs, acc, 'r')\n",
        "    plt.plot(epochs, val_acc, 'b')\n",
        "    plt.title('Training and validation accuracy')\n",
        "    plt.legend(['train', 'val'], loc='lower right')\n",
        "    #plt.show()\n",
        "    plt.savefig(accLoc)\n",
        "\n",
        "\n",
        "    return model_glove"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsCpu62nSDB9"
      },
      "source": [
        "from keras.models import Sequential,load_model\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing import sequence\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import pickle\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import gc\n",
        "import keras.backend as K\n",
        "\n",
        "###########################################################################\n",
        "vocabulary_size = 400000\n",
        "\n",
        "time_step=300\n",
        "\n",
        "dataset=pickle.load(open(project_path+\"Train_Test_Data/train_df.pkl\",\"rb\"))\n",
        "texts=[]\n",
        "#texts=dataset['Statement'].astype(str).values.tolist()\n",
        "\n",
        "texts=dataset['text']\n",
        "#texts=texts.apply(str)\n",
        "#texts=texts.map(lambda x: clean_text(x))\n",
        "\n",
        "label=dataset['labels'].astype(int).values.tolist()\n",
        "labelEncoder=LabelEncoder()\n",
        "encoded_label=labelEncoder.fit_transform(label)\n",
        "y=np.reshape(encoded_label,(-1,1))\n",
        "\n",
        "tokenizer_train=Tokenizer(num_words=vocabulary_size)\n",
        "tokenizer_train.fit_on_texts(texts)\n",
        "encoded_train=tokenizer_train.texts_to_sequences(texts=texts)\n",
        "#print(encoded_docs)\n",
        "vocab_size_train = len(tokenizer_train.word_index) + 1\n",
        "print(vocab_size_train)\n",
        "\n",
        "X = sequence.pad_sequences(encoded_train, maxlen=time_step,padding='post')\n",
        "\n",
        "\n",
        "f = open(project_path+'glove.6B.100d.txt',encoding='utf-8')\n",
        "embeddings_train={}\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_train[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Total %s word vectors.' % len(embeddings_train))\n",
        "\n",
        "embedding_size=100\n",
        "\n",
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = np.zeros((vocab_size_train, embedding_size))\n",
        "for word, i in tokenizer_train.word_index.items():\n",
        "    embedding_vector_train = embeddings_train.get(word)\n",
        "    if embedding_vector_train is not None:\n",
        "        embedding_matrix[i] = embedding_vector_train\n",
        "\n",
        "\n",
        "dataset=pickle.load(open(project_path+\"Train_Test_Data/test_df.pkl\",\"rb\"))\n",
        "statement=dataset.iloc[:,2:3].values\n",
        "#statement=statement.lower()\n",
        "texts=[]\n",
        "\n",
        "texts=dataset['text']\n",
        "\n",
        "#texts=texts.map(lambda x: clean_text(x))\n",
        "\n",
        "label=dataset['labels'].astype(int).values.tolist()\n",
        "labelEncoder=LabelEncoder()\n",
        "encoded_label=labelEncoder.fit_transform(label)\n",
        "y_test=np.reshape(encoded_label,(-1,1))\n",
        "\n",
        "encoded_test=tokenizer_train.texts_to_sequences(texts=texts)\n",
        "X_test = sequence.pad_sequences(encoded_test, maxlen=time_step, padding='post')\n",
        "\n",
        "vocab_size=embedding_matrix.shape[0]\n",
        "##########################################################################################\n",
        "\n",
        "\n",
        "\n",
        "Fold = 1\n",
        "\n",
        "gc.collect()\n",
        "K.clear_session()\n",
        "print('Fold: ', Fold)\n",
        "\n",
        "X_train = X\n",
        "\n",
        "\n",
        "y_train = y\n",
        "\n",
        "\n",
        "# create model\n",
        "print(\"Creating and Fitting Model...\")\n",
        "model = create_model(vocabulary_size=vocab_size,embedding_size=embedding_size,embedding_matrix=embedding_matrix)\n",
        "\n",
        "history=model.fit(X_train, y_train, epochs=10, batch_size=128,shuffle=True)##############\n",
        "\n",
        "\n",
        "# evaluate the model\n",
        "print(\"Evaluating Model...\")\n",
        "##########################################\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Eval %s: %.2f%%\" % (model.metrics_names[1], scores[1]))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1dfXsM7g6c9"
      },
      "source": [
        "from sklearn.metrics import precision_recall_fscore_support, classification_report\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred = np.round(y_pred).astype(int)\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}